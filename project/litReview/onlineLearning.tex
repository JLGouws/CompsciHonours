\subsection{Online Learning}
  Online learning is learning done as data becomes available.
  This form of model training is required to handle streaming data, for example, stock prices and videos.
  Offline learning can also use online learning methods.
  This technique is applicable, for example, when a dataset is too big to load into a computer's memory.

  One option for training a classifier online is Stochastic Gradient Descent(SGD).
  SGD aims to minimise a cost function given by: 
  \begin{equation*}
    J(\pmb{\theta}) = J^*\left(f(\pmb{\theta}, \pmb{z}_1), f(\pmb{\theta}, \pmb{z}_2),\hdots, f(\pmb{\theta}, \mathbf{z}_N))\right)
  \end{equation*}
  Changing the model's parameters $\pmb{\theta}$ achieves minimisation \cite{murphy2012}.
  The $f(\pmb{\theta}, \pmb{z}_i)$ are functions that give the cost of some data point $\pmb{z}_i$ for the parameters $\pmb{\theta}$
  SGD updates the parameters in sequential steps of the form:
  \begin{equation*}
    \pmb{\theta}_{k + 1} = G(\pmb{\theta}_k, \nabla f(\pmb{\theta}, z_k))
  \end{equation*}
  There are many ways to implement gradient descent, and adaptive step sizes can improve the optimisation results \cite{duchi2011}.

  Another option for an online learning classifier uses Bayesian inference.
  Training the Bayesian classifier updates its posterior probability according to:
  \begin{equation*}
    p(\pmb{\theta} | \mathcal{D}_{1:k}) \propto p(\mathcal{D}_k | \pmb{\theta})p(\pmb{\theta} | \mathcal{D}_{1:k-1})
  \end{equation*}
  Where $\mathcal{D}_{1:n}$ indicates that the classifier has been given data points $1$ to $n$.
  Bayes classifiers, in most cases, can be trained faster than classifiers using SGD \cite{murphy2012}.
