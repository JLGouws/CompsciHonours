\subsection{Online Learning}
  One option for solving the problem of online learning is Stochastic Gradient Descent.
  Stochastic gradient descent aims to minimize a cost function, $f(\mathbf{\theta})$, by changing the parameters, $\mathbf{\theta}$.
  The parameters are updated in sequential steps of the form:
  \begin{equation*}
    \mathbf{\theta}_{k + 1} = G(\mathbf{\theta}, f())
  \end{equation*}
