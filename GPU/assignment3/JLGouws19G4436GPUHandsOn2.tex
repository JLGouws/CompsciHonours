\documentclass[a4paper,12pt]{article}
\usepackage{amsmath,amsfonts,amsthm,amssymb, mathtools,steinmetz, gensymb, siunitx}	% LOADS USEFUL MATH STUFF
\usepackage{xcolor,graphicx}
<<<<<<< HEAD
\usepackage[left=2cm, top=2cm, right=2cm, bottom=1cm ,a4paper]{geometry} 				% ADJUSTS PAGE
=======
\usepackage[left=2cm, top=2cm, right=2cm, bottom=2cm ,a4paper]{geometry} 				% ADJUSTS PAGE
>>>>>>> 1852523aabafe7070bd9600244e2fa50038b0e24
\usepackage{setspace}
\usepackage{caption}
\usepackage{tikz}
\usepackage{pgf,tikz,pgfplots}
\usepackage{mathrsfs}
\usepackage{fancyhdr}
\usepackage{float}
\usepackage{array}
\usepackage{unicode-math}
\usepackage{booktabs}
\usepackage{multirow}
\setmathfont{Libertinus Math}

\usetikzlibrary{decorations.pathreplacing,decorations.markings}
\usepgfplotslibrary{fillbetween}

\newcommand{\defeq}{\vcentcolon=}
\newcommand\block[1]{\hspace*{#1}}
\newcommand{\rpm}{\sbox0{$1$}\sbox2{$\scriptstyle\pm$}
	  \raise\dimexpr(\ht0-\ht2)/2\relax\box2 }
\pgfplotsset{compat=1.11}
	  
\newlength{\QNo}
\settowidth{\QNo}{2.}

\newlength{\QLetter}
\settowidth{\QLetter}{(a)}

\pagestyle{fancy}
\rhead{CS Honours GPUs}
\lhead{J L Gouws}

\begin{document}
\fontencoding{T1}
\fontfamily{ppl}\selectfont
\thispagestyle{empty}

{\Large \textbf{GPUs Hands On 2}} \hfill {\Large \textbf{J L Gouws}}\\
\block{1.0cm} {\large \textbf{\today}} \hfill {\large \textbf{19G4436}}\\

<<<<<<< HEAD
  Each kernel runs 1000 times; Each kernel runs exclusively on the device. The program finds the average run time.
  \begin{table}[H]
    \resizebox{\textwidth}{!}{
    \begin{tabular}{|c|c|c|c|c|c|c|}
      \hline
      \textbf{2D Block} & \multicolumn{2}{c|}{\textbf{Step 3}} & \multicolumn{2}{c|}{\textbf{Step 4}} & \multicolumn{2}{c|}{\textbf{Step 5}}  \\ \hline
               & 2D Grid Size& \multicolumn{1}{p{5em}|}{Execution \newline Time ms} & \multicolumn{1}{p{5.5em}|}{1D Grid size\newline 1 datum per thread} & \multicolumn{1}{p{5.5em}|}{Execution time ms}&\multicolumn{1}{p{5em}|}{Grid Size\newline 16 data per thread} & \multicolumn{1}{p{5em}|}{Execution time ms} \\ \hline
      $32 \times 32$ & $128 \times 128$ & 2.725741 & 16384 & 2.691122 & 1024 & 2.777440 \\ \hline
      $32 \times 16$ & $128 \times 256$ & 2.694830 & 32768 & 2.704954 & 2048 & 2.790173 \\ \hline
      $16 \times 32$ & $256 \times 128$ & 2.707887 & 32768 & 2.708225 & 2048 & 2.788703 \\ \hline
      $16 \times 16$ & $256 \times 256$ & 2.686316 & 65536 & 2.711797 & 4096 & 2.792496 \\ \hline
      $64 \times 16$ & $64  \times 256$ & 2.701534 & 16384 & 2.698435 & 1024 & 2.785574 \\ \hline
      $16 \times 64$ & $256 \times 64 $ & 2.825843 & 16384 & 2.696335 & 1024 & 2.788394 \\ \hline
      $64 \times  8$ & $64  \times 512$ & 2.669709 & 32768 & 2.711425 & 2048 & 2.803214 \\ \hline
      $8 \times  64$ & $512  \times 64$ & 3.444676 & 32768 & 2.717273 & 2048 & 2.790220 \\ \hline
      $64 \times  2$ & $64 \times 2048$ & 2.715393 & 131072& 2.725050 & 8192 & 2.825520 \\ \hline
      $2 \times  64$ & $2048 \times 64$ & 5.792675 & 131072& 2.726640 & 8192 & 2.835794 \\ \hline
    \end{tabular}
  }
    \caption{Average execution times of Kernel}
    \label{table2}
  \end{table}
  Observations:\\
  On average, the performance seems to be worse with larger block size.
  Larger block sizes result in fewer blocks being launched.
  Fewer blocks being launched can result in unoccupied streaming mulitprocessors.
  Less usage of the streaming mulitprocessors results in fewer FLOPs--that is lower performance.

  When each thread does 16 data items per thread, the threads must stride accross the data in order to achieve decent performance.
  If the threads do not stride over  the data, the kernel performs poorly.
  This performance reduction is due to memory access in the GPU.
  I think that striding through the data allows the kernel to use the spatial locality of cache.
  This use of the cache's spatial locality increases performance, because new data items do not have to be fetched from global memory for every execution of one or two warps.

  With striding, all the kernels seem to perform similarly.
  The normal one dimensional grid seems to perform the best on average.
  The one dimensional grid with 16 data per thread seems to perform the worst on average.
  The two dimensional grid has the most variation with a few outliers.

  The execution times are not symmetric with block sizes.
  The $2D$ grid kernel performs fine with a $64 \times 2$ block size, but poorly with a $2 \times 64$ block size.
  This might be attributed to warp divergence.
  I should use the profiler to confirm this.
=======
Each kernel ran 1000 times. 
The average runtime was found from NVVP.
This usage of the profiler is rather bad.
I should have coded the program so that it works out the average execution time for each kernel.
\begin{table}[!h]
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
    \textbf{2D Block} & \multicolumn{2}{c|}{\textbf{Step 3}} & \multicolumn{2}{c|}{\textbf{Step 4}} & \multicolumn{2}{c|}{\textbf{Step 5}}  \\ \hline
             & 2D Grid Size& \multicolumn{1}{p{5em}|}{Execution \newline Time ms} & \multicolumn{1}{p{5.5em}|}{1D Grid size\newline 1 datum per thread} & \multicolumn{1}{p{5.5em}|}{Execution time ms}&\multicolumn{1}{p{5em}|}{Grid Size\newline 16 data per thread} & \multicolumn{1}{p{5em}|}{Execution time ms} \\ \hline
    $32 \times 32$ & $128 \times 128$ & 2.90074 & 16384 & 2.7162 & 1024 & 2.7919 \\ \hline
    $32 \times 16$ & $128 \times 256$ & 2.72323 & 32768 & 2.70327 & 2048 & 2.7999 \\ \hline
    $16 \times 32$ & $256 \times 128$ & 2.78609 & 32768 & 2.70757 & 2048 & 2.80921 \\ \hline
    $16 \times 16$ & $256 \times 256$ & 2.75102 & 65536 & 2.72549 & 4096 & 2.80999 \\ \hline
    $64 \times 16$ & $64  \times 256$ & 2.89524 & 16384 & 2.71913 & 1024 & 2.80161 \\ \hline
    $16 \times 64$ & $256 \times 64 $ & 2.92079 & 16384 & 2.71883 & 1024 & 2.83459 \\ \hline
    $64 \times  8$ & $64  \times 512$ & 2.71029 & 32768 & 2.74134 & 2048 & 2.82161 \\ \hline
  \end{tabular}
  \caption{Average execution times of Kernel}
  \label{table2}
\end{table}
Observations:\\
I have realized that I only recorded three of my own block sizes; I do not have access to my Lab Computer at the time of writing this.
Please excuse the lack of extra experiments.

On average, the performance seems to be worse with larger block sizes.
Larger block sizes result in fewer blocks being launched.
Fewer blocks being launched can result in unoccupied streaming multiprocessors.
Less usage of the streaming multiprocessors results in fewer FLOPS--that is lower performance.

When doing 16 data items per thread, the threads must stride across the data in order to acheive decent performance.
If no striding is done, the kernel performs poorly.
This performance reduction is due to memory access in the GPU.
I think that striding through the data allows the kernel to use the spatial locality of cache.
This use of the cache's spatial locality would increase perfomance, because new data items do not have to be fetched from gobal memory every time one or two warps are executed.

With striding, all the kernels seem to perform similarly.
The sixteen data per thread category seems to perform the worst on average.
The one dimensional grid seems to perform the best on average.


>>>>>>> 1852523aabafe7070bd9600244e2fa50038b0e24
\end{document}
