\documentclass[a4paper,12pt]{article}
\usepackage{amsmath,amsfonts,amsthm,amssymb, mathtools,steinmetz, gensymb, siunitx}	% LOADS USEFUL MATH STUFF
\usepackage{xcolor,graphicx}
\usepackage[left=2cm, top=2cm, right=2cm, bottom=2cm ,a4paper]{geometry} 				% ADJUSTS PAGE
\usepackage{setspace}
\usepackage{caption}
\usepackage{tikz}
\usepackage{pgf,tikz,pgfplots}
\usepackage{mathrsfs}
\usepackage{fancyhdr}
\usepackage{float}
\usepackage{array}
\usepackage{unicode-math}
\usepackage{booktabs}
\usepackage{multirow}
\setmathfont{Libertinus Math}

\usetikzlibrary{decorations.pathreplacing,decorations.markings}
\usepgfplotslibrary{fillbetween}

\newcommand{\defeq}{\vcentcolon=}
\newcommand\block[1]{\hspace*{#1}}
\newcommand{\rpm}{\sbox0{$1$}\sbox2{$\scriptstyle\pm$}
	  \raise\dimexpr(\ht0-\ht2)/2\relax\box2 }
\pgfplotsset{compat=1.11}
	  
\newlength{\QNo}
\settowidth{\QNo}{2.}

\newlength{\QLetter}
\settowidth{\QLetter}{(a)}

\pagestyle{fancy}
\rhead{CS Honours GPUs}
\lhead{J L Gouws}

\begin{document}
\fontencoding{T1}
\fontfamily{ppl}\selectfont
\thispagestyle{empty}

{\Large \textbf{GPUs Hands On 2}} \hfill {\Large \textbf{J L Gouws}}\\
\block{1.0cm} {\large \textbf{\today}} \hfill {\large \textbf{19G4436}}\\

Each kernel ran 1000 times. 
The average runtime was found from NVVP.
This usage of the profiler is rather bad.
I should have coded the program so that it works out the average execution time for each kernel.
\begin{table}[!h]
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
    \textbf{2D Block} & \multicolumn{2}{c|}{\textbf{Step 3}} & \multicolumn{2}{c|}{\textbf{Step 4}} & \multicolumn{2}{c|}{\textbf{Step 5}}  \\ \hline
             & 2D Grid Size& \multicolumn{1}{p{5em}|}{Execution \newline Time ms} & \multicolumn{1}{p{5.5em}|}{1D Grid size\newline 1 datum per thread} & \multicolumn{1}{p{5.5em}|}{Execution time ms}&\multicolumn{1}{p{5em}|}{Grid Size\newline 16 data per thread} & \multicolumn{1}{p{5em}|}{Execution time ms} \\ \hline
    $32 \times 32$ & $128 \times 128$ & 2.90074 & 16384 & 2.7162 & 1024 & 2.7919 \\ \hline
    $32 \times 16$ & $128 \times 256$ & 2.72323 & 32768 & 2.70327 & 2048 & 2.7999 \\ \hline
    $16 \times 32$ & $256 \times 128$ & 2.78609 & 32768 & 2.70757 & 2048 & 2.80921 \\ \hline
    $16 \times 16$ & $256 \times 256$ & 2.75102 & 65536 & 2.72549 & 4096 & 2.80999 \\ \hline
    $64 \times 16$ & $64  \times 256$ & 2.89524 & 16384 & 2.71913 & 1024 & 2.80161 \\ \hline
    $16 \times 64$ & $256 \times 64 $ & 2.92079 & 16384 & 2.71883 & 1024 & 2.83459 \\ \hline
    $64 \times  8$ & $64  \times 512$ & 2.71029 & 32768 & 2.74134 & 2048 & 2.82161 \\ \hline
  \end{tabular}
  \caption{Average execution times of Kernel}
  \label{table2}
\end{table}
Observations:\\
I have realized that I only recorded three of my own block sizes; I do not have access to my Lab Computer at the time of writing this.
Please excuse the lack of extra experiments.

On average, the performance seems to be worse with larger block sizes.
Larger block sizes result in fewer blocks being launched.
Fewer blocks being launched can result in unoccupied streaming multiprocessors.
Less usage of the streaming multiprocessors results in fewer FLOPS--that is lower performance.

When doing 16 data items per thread, the threads must stride across the data in order to acheive decent performance.
If no striding is done, the kernel performs poorly.
This performance reduction is due to memory access in the GPU.
I think that striding through the data allows the kernel to use the spatial locality of cache.
This use of the cache's spatial locality would increase perfomance, because new data items do not have to be fetched from gobal memory every time one or two warps are executed.

With striding, all the kernels seem to perform similarly.
The sixteen data per thread category seems to perform the worst on average.
The one dimensional grid seems to perform the best on average.


\end{document}
