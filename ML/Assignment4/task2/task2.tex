\begin{enumerate}
  \item Machine learning is the study of finding or making a function $f$ that takes input $x$ from a set $D$ and gives output $y$, where nothing is known about the relationship between $x$ and $y$ a priori.
        Supervised learning is the case where $y$ is known for some subset of $D$.
        Unsupervised learning is the case where nothing is known about the ouput of the function.
        An example would be taking a picture, $x$ of a dog and applying $f$ to classify, $y$, the picture as a picture of a dog.
  \item
      \begin{enumerate}
        \item Feature scaling is changing the values of (numeric)features so that they conform to some restriction, without significantly changing the information encoded within the features.
        \item Distance based learning algorithms.
          \begin{itemize}
            \item K Nearest Neighbors
            \item Neural networks
          \end{itemize}
        \item Tree based algorithms
          \begin{itemize}
            \item Random forest.
            \item Random forest.
          \end{itemize}
      \end{enumerate}
    \item
      \begin{enumerate}
        \item
        \item
          \begin{description}
              \item[Advantages] It gives a better estimation of the average generalization of the model.
              \item[Disadvantages] It takes a long time to run.
                                   This is a less significant problem, but more data is generally needed.
          \end{description}
      \end{enumerate}
    \item
      \begin{enumerate}
          \item
            The gradient descent method might only find a local minimum and not the global minimum.
          \item

          \item
            By looking at the error data, overfitting of the model can be seen.
            If the model is overfitting then more regularization is needed.
            If the model is tarting to underfit then less regularization is needed.
      \end{enumerate}
\end{enumerate}
